{"ast":null,"code":"import _asyncToGenerator from \"/Users/bharris/Code/audio/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport { signal } from '@angular/core';\nimport * as ort from 'onnxruntime-web';\nimport { getModel } from './model-registry';\nimport * as i0 from \"@angular/core\";\nexport let OnnxSeparatorService = /*#__PURE__*/(() => {\n  class OnnxSeparatorService {\n    constructor() {\n      this.session = null;\n      this.currentModel = signal(null);\n      this.progress = signal(null);\n    }\n    loadModel(id) {\n      var _this = this;\n      return _asyncToGenerator(function* () {\n        const info = getModel(id);\n        if (!info) throw new Error('Model not found');\n        _this.currentModel.set(info);\n        _this.progress.set({\n          stage: 'downloading',\n          message: 'Fetching model...'\n        });\n        // onnxruntime-web handles fetching inside createInferenceSession if given URL\n        _this.progress.set({\n          stage: 'initializing',\n          message: 'Creating session...'\n        });\n        _this.session = yield ort.InferenceSession.create(info.url, {\n          executionProviders: ['wasm'],\n          graphOptimizationLevel: 'all'\n        });\n      })();\n    }\n    ensureSession() {\n      if (!this.session || !this.currentModel()) throw new Error('Model not loaded');\n    }\n    separate(buffer, opts) {\n      var _this2 = this;\n      return _asyncToGenerator(function* () {\n        _this2.ensureSession();\n        const model = _this2.currentModel();\n        const session = _this2.session;\n        if (model.requiresStereo && buffer.numberOfChannels < 2) {\n          throw new Error('Model requires stereo audio');\n        }\n        // Resample if needed (simple offline context resample)\n        let workBuffer = buffer;\n        if (buffer.sampleRate !== model.sampleRate) {\n          const offline = new OfflineAudioContext(buffer.numberOfChannels, Math.floor(buffer.duration * model.sampleRate), model.sampleRate);\n          const src = offline.createBufferSource();\n          src.buffer = buffer;\n          src.connect(offline.destination);\n          src.start();\n          workBuffer = yield offline.startRendering();\n        }\n        const totalLength = workBuffer.length;\n        const chunkSize = model.chunkSize;\n        const chunkCount = Math.ceil(totalLength / chunkSize);\n        const stems = {};\n        model.stems.forEach(stem => stems[stem] = []);\n        for (let ci = 0; ci < chunkCount; ci++) {\n          const start = ci * chunkSize;\n          const end = Math.min(start + chunkSize, totalLength);\n          const sliceLen = end - start;\n          // Prepare input tensor shape: [batch, channels, samples]\n          const channels = 2; // Using first two channels\n          const inputData = new Float32Array(channels * sliceLen);\n          for (let ch = 0; ch < channels; ch++) {\n            const srcChan = workBuffer.getChannelData(ch);\n            for (let i = 0; i < sliceLen; i++) {\n              inputData[ch * sliceLen + i] = srcChan[start + i];\n            }\n          }\n          const inputTensor = new ort.Tensor('float32', inputData, [1, channels, sliceLen]);\n          _this2.progress.set({\n            stage: 'chunk',\n            chunkIndex: ci,\n            chunkCount,\n            message: `Processing chunk ${ci + 1}/${chunkCount}`\n          });\n          opts?.onProgress?.(_this2.progress());\n          // Run inference (placeholder – actual output names depend on model)\n          // Assume outputs: stem_0, stem_1, ... matching model.stems order\n          const feeds = {\n            'input': inputTensor\n          };\n          let results;\n          try {\n            results = yield session.run(feeds);\n          } catch (e) {\n            throw new Error('Inference failed: ' + e.message);\n          }\n          model.stems.forEach((stem, idx) => {\n            const key = `stem_${idx}`;\n            const out = results[key];\n            if (!out) return; // Skip missing\n            const data = out.data;\n            // Expect shape [1, channels, sliceLen]\n            const samplesPerChannel = data.length / channels;\n            for (let ch = 0; ch < channels; ch++) {\n              const chanSlice = data.subarray(ch * samplesPerChannel, (ch + 1) * samplesPerChannel);\n              stems[stem][ch] = stems[stem][ch] ? concatFloat32(stems[stem][ch], chanSlice) : new Float32Array(chanSlice);\n            }\n          });\n        }\n        // Convert collected stem channel arrays to final output arrays\n        const outputs = model.stems.map(stem => {\n          const chanArrays = stems[stem];\n          // Ensure both channels present\n          const channelData = [];\n          for (let ch = 0; ch < 2; ch++) {\n            channelData.push(chanArrays[ch] || new Float32Array(0));\n          }\n          return {\n            name: stem,\n            audio: channelData\n          };\n        });\n        _this2.progress.set({\n          stage: 'done',\n          message: 'Separation complete'\n        });\n        opts?.onProgress?.(_this2.progress());\n        return outputs;\n      })();\n    }\n    static {\n      this.ɵfac = function OnnxSeparatorService_Factory(t) {\n        return new (t || OnnxSeparatorService)();\n      };\n    }\n    static {\n      this.ɵprov = /*@__PURE__*/i0.ɵɵdefineInjectable({\n        token: OnnxSeparatorService,\n        factory: OnnxSeparatorService.ɵfac,\n        providedIn: 'root'\n      });\n    }\n  }\n  return OnnxSeparatorService;\n})();\nfunction concatFloat32(a, b) {\n  const out = new Float32Array(a.length + b.length);\n  out.set(a, 0);\n  out.set(b, a.length);\n  return out;\n}","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}